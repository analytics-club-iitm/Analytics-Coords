{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TransferLearning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPulDfv3nRd+1nHL4P9ZDcN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wigglytuff-tu/Analytics-Coords/blob/main/TransferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXLQD31byH73"
      },
      "source": [
        "### **Transfer Learning**\n",
        "In this Notebook we will go through the process of Transfer learning, writing the code from scratch for an Image Classification Task. We will we be using the following dataset: https://www.kaggle.com/itsahmad/indoor-scenes-cvpr-2019/code. Since most categories have only 100 images which typically isn’t enough for a neural network to learn to high accuracy. Therefore, instead of building and training a CNN from scratch, we’ll use a pre-built and pre-trained model applying transfer learning. we use a model which has been trained on a larger dataset and use the pre-trained to predict over our smaller dataset.The idea is the convolutional layers extract general, low-level features that are applicable across images — such as edges, patterns, gradients — and the later layers identify specific features within an image such as eyes or wheels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqaCf6kZyGrz"
      },
      "source": [
        "#Import the Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAYRPAdA10Gc"
      },
      "source": [
        "#Getting the Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsbXbCnp2HFM",
        "outputId": "1dbfa2b4-6d43-4bfe-ee1c-f28c49ce4149"
      },
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\" #Directing the environment to kaggle Folder\n",
        "%cd /content/gdrive/My Drive/Kaggle"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Kaggle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIGq_S7l2T3B",
        "outputId": "857ffe7c-7556-4ec2-b5dd-fe7aea951c34"
      },
      "source": [
        "!kaggle datasets download -d itsahmad/indoor-scenes-cvpr-2019"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading indoor-scenes-cvpr-2019.zip to /content/gdrive/My Drive/Kaggle\n",
            "100% 2.33G/2.34G [00:36<00:00, 70.8MB/s]\n",
            "100% 2.34G/2.34G [00:36<00:00, 69.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qQOBieS2hzE"
      },
      "source": [
        "!ls\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXsJ5b-B27LZ",
        "outputId": "9956500c-dd29-4bf5-9652-d47356fc57f1"
      },
      "source": [
        "data_dir  = './indoorCVPR_09/Images'\n",
        "classes = os.listdir(data_dir)\n",
        "print(classes)\n",
        "len(classes)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['airport_inside', 'artstudio', 'auditorium', 'bakery', 'bar', 'bathroom', 'bedroom', 'bookstore', 'bowling', 'buffet', 'casino', 'children_room', 'church_inside', 'classroom', 'cloister', 'closet', 'clothingstore', 'computerroom', 'concert_hall', 'corridor', 'deli', 'dentaloffice', 'dining_room', 'elevator', 'fastfood_restaurant', 'florist', 'gameroom', 'garage', 'greenhouse', 'grocerystore', 'gym', 'hairsalon', 'hospitalroom', 'inside_bus', 'inside_subway', 'jewelleryshop', 'kindergarden', 'kitchen', 'laboratorywet', 'laundromat', 'library', 'livingroom', 'lobby', 'locker_room', 'mall', 'meeting_room', 'movietheater', 'museum', 'nursery', 'office', 'operating_room', 'pantry', 'poolinside', 'prisoncell', 'restaurant', 'restaurant_kitchen', 'shoeshop', 'stairscase', 'studiomusic', 'subway', 'toystore', 'trainstation', 'tv_studio', 'videostore', 'waitingroom', 'warehouse', 'winecellar']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvJnSsD130ny"
      },
      "source": [
        "We are done with the first step, i.e. importing our Dataset. Now we will preprocess our dataset to use with our pre-trained ImageNet Model.Each model will have different input requirements, but if we read through what Imagenet requires, we figure out that our images need to be 224x224 and normalized to a range.To process an image in PyTorch, we use ```transforms``` , simple operations applied to arrays. The validation (and testing) transforms are as follows:\n",
        "\n",
        "\n",
        "*   Resize\n",
        "\n",
        "*   Center crop to 224 x 224\n",
        "*   Convert to a tensor\n",
        "\n",
        "\n",
        "*   Normalize with mean and standard deviation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXXOZ_tJ6ZKk"
      },
      "source": [
        "# Image transformations\n",
        "image_transforms = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.ColorJitter(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=224),  # Image net standards\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])  # Imagenet standards\n",
        "    ])\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUMUXmqU63bG"
      },
      "source": [
        "#Creating Datasets and Dataloaders\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# Datasets from folders\n",
        "data = datasets.ImageFolder(data_dir, transform=image_transforms)    \n",
        "\n",
        "train_ds, val_ds, test_ds = torch.utils.data.random_split(data, [13500, 1500, 620])\n",
        "len(train_ds), len(val_ds), len(test_ds)\n",
        "\n",
        "# Dataloader iterators, make sure to shuffle\n",
        "dataloaders = {\n",
        "    'train': DataLoader(train_ds, batch_size=32, shuffle=True),\n",
        "    'val': DataLoader(val_ds, batch_size=32, shuffle=True)\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEFqVqAV-Ztd"
      },
      "source": [
        "# Iterate through the dataloader once\n",
        "'''trainiter = iter(dataloaders['train'])\n",
        "features, labels = next(trainiter)\n",
        "features.shape, labels.shape'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nvVGjkU--L0"
      },
      "source": [
        "# **Pre-Trained Models for Image Recognition**\n",
        "With our data in shape, we next turn our attention to the model. For this, we’ll use a pre-trained convolutional neural network. PyTorch has a number of models that have already been trained on millions of images from 1000 classes in [Imagenet](http://www.image-net.org/). The complete list of models can be [seen here](https://pytorch.org/docs/stable/torchvision/models.html). The performance of these models on Imagenet is shown below:\n",
        "\n",
        "![image.png](https://miro.medium.com/max/942/1*0W310-cMNHPWjErqPuGXpw.png)\n",
        "\n",
        "Here, we will use Inception v3. Now we will follow these steps:\n",
        "\n",
        "1.   Load in pre-trained weights from a network trained on a large dataset\n",
        "2.   Freeze all the weights in the lower (convolutional) layers: the layers to freeze are adjusted depending on similarity of new task to original dataset\n",
        "\n",
        "1.   Replace the upper layers of the network with a custom classifier: the number of outputs must be set equal to the number of classes (here 67 classes)\n",
        "2.   Train only the custom classifier layers for the task thereby optimizing the model for smaller dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUbjlt3GAyLn"
      },
      "source": [
        "#Loading in pre-trained Model\n",
        "from torchvision import models\n",
        "model = models.inception_v3(pretrained = True)\n",
        "\n",
        "#You can use ```dir(models)``` to see various models available for Transfer Learning"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0PeNh-tLv15"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bgXA09yDnO4"
      },
      "source": [
        "# Freeze model weights\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "#print(model)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4JvV_X6ELVk"
      },
      "source": [
        "#Add the following layer with 67 output fetures as we have 67 classes\n",
        "\n",
        "\n",
        "num_ftrs = model.AuxLogits.fc.in_features\n",
        "model.AuxLogits.fc = nn.Linear(num_ftrs, 67)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 67)\n",
        "model = model.to('cuda')  #Moving Model to GPU"
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}
