{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOCy+HtfEt5R4ZiYUCis67R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wigglytuff-tu/Analytics-Coords/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dGO-qwn-zxW"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxBwT8lc3_mw",
        "outputId": "e5a83909-a19c-4650-a6e5-c2300ca5803d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwXBBVFm4WB1",
        "outputId": "1a356dba-46f1-4f02-881b-2ebc4fd323b4"
      },
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "%cd /content/gdrive/My Drive/Kaggle"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Kaggle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UC43bl349U9",
        "outputId": "2016e9ec-91fd-46e7-a997-db95e9defe8b"
      },
      "source": [
        "!kaggle datasets download -d itsahmad/indoor-scenes-cvpr-2019"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading indoor-scenes-cvpr-2019.zip to /content/gdrive/My Drive/Kaggle\n",
            "100% 2.33G/2.34G [00:32<00:00, 101MB/s] \n",
            "100% 2.34G/2.34G [00:32<00:00, 76.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts_tFP005Non"
      },
      "source": [
        "!ls\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUYNvRGq6Una",
        "outputId": "c1607d87-803d-468e-a155-ffeb1492d7b0"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "indoorCVPR_09\t\t  indoor-scenes-cvpr-2019.zip  TestImages.txt\n",
            "indoorCVPR_09annotations  kaggle.json\t\t       TrainImages.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8W9KYlm6Xqb"
      },
      "source": [
        "data_dir  = './indoorCVPR_09/Images'\n",
        "classes = os.listdir(data_dir)\n",
        "print(classes)\n",
        "len(classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPUProswQtqE"
      },
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transformations = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor(),transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "dataset = ImageFolder(data_dir, transform = transformations)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXcg2WvFQ1FV",
        "outputId": "1270e07d-4f7b-4391-c71f-74e6fe1902da"
      },
      "source": [
        "import matplotlib.pyplot as plt   \n",
        "%matplotlib inline\n",
        "\n",
        "def show_sample(img, label):\n",
        "    print(\"Label:\", dataset.classes[label], \"(Class No: \"+ str(label) + \")\")\n",
        "    plt.imshow(img.permute(1, 2, 0))\n",
        "\n",
        "train_ds, val_ds, test_ds = torch.utils.data.random_split(dataset, [13500, 1500, 620])\n",
        "len(train_ds), len(val_ds), len(test_ds)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13500, 1500, 620)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzVM9whR_bv_"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout2d(0.2)\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout2d(0.3)\n",
        "        )\n",
        "\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128)\n",
        "        )\n",
        "        self.conv6 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout2d(0.4)\n",
        "        )\n",
        "        self.conv7 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256)\n",
        "        )\n",
        "        self.conv8 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Dropout2d(0.4)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(256, 67)  # number of classes = 67\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv8(self.conv7(self.conv6(self.conv5(self.conv4(self.conv3(self.conv2(self.conv1(x))))))))\n",
        "        \n",
        "        x = F.avg_pool2d(x, kernel_size=x.shape[2:])\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH_NIQxqNoeL"
      },
      "source": [
        "def train(model, dataloader, epoch, optimizer, criterion):\n",
        "         \n",
        "      model.train()\n",
        "      train_loss = []\n",
        "      train_acc = []\n",
        "      for batch_idx, (data, target) in enumerate(dataloader):\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "          output = model(data)\n",
        "          \n",
        "          optimizer.zero_grad()    \n",
        "          loss = criterion(output, target)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "          pred_cls = output.max(1)[1]\n",
        "          correct = pred_cls.eq(target.long().data).cpu().sum()\n",
        "          \n",
        "          train_acc.append(correct.item()/data.shape[0])\n",
        "          train_loss.append(loss.item())\n",
        "          \n",
        "          if batch_idx % 1 == 0:\n",
        "              print('Train Epoch: {} [({:.0f}%)]\\tTrain_Loss: {:.4f}\\tTrain_Acc: {:.4f}%'.format(\n",
        "              epoch+1, 100. * batch_idx / len(dataloader), np.mean(train_loss), 100*np.mean(train_acc)))\n",
        "      print()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diaW5rorNs-i"
      },
      "source": [
        "def evaluate_model(model, testdataloader):\n",
        "\n",
        "      val_acc = []\n",
        "      val_loss = []\n",
        "      model.eval()\n",
        "      \n",
        "      with torch.no_grad():\n",
        "          for data, target in testdataloader:\n",
        "              data, target = data.cuda(), target.cuda()\n",
        "              output = model(data)\n",
        "              pred_cls = output.max(1)[1]\n",
        "              correct = pred_cls.eq(target.long().data).cpu().sum()\n",
        "              val_acc.append(correct.item()/data.shape[0])\n",
        "              \n",
        "      print(\"Val Acc: {:.4f}%\".format(100*np.mean(val_acc)))\n",
        "      return np.mean(val_acc)    "
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1s6F7PAKXWx"
      },
      "source": [
        "def main():\n",
        "    \n",
        "   # Defining the dataloader\n",
        "    \n",
        "    train_dl = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle = True)\n",
        "    \n",
        "    val_dl = torch.utils.data.DataLoader(val_ds, batch_size=32, shuffle = False)\n",
        "    \n",
        "    # defining the model\n",
        "    model = CNN()\n",
        "    # defining the optimizer\n",
        "    optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    # defining the loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "    \n",
        "    print(\"Structure of the Model\",model)\n",
        "    \n",
        "    #Training the model \n",
        "    n_epochs = 25\n",
        "    best = 0\n",
        "    for epoch in range(n_epochs):\n",
        "        train(model, train_dl, epoch, optimizer, criterion)\n",
        "        val_acc = evaluate_model(model, val_dl)\n",
        "        if val_acc > best:\n",
        "            best = val_acc\n",
        "            #torch.save(model.state_dict(), \"best.ckpt\")\n",
        "    \n",
        "    print(\"Training Finished\")"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE3QidXUM-ix",
        "outputId": "0de3b534-5cc1-4306-9bb7-e9afc067d2f3"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 3 [(91%)]\tTrain_Loss: 3.3236\tTrain_Acc: 15.3564%\n",
            "Train Epoch: 3 [(91%)]\tTrain_Loss: 3.3224\tTrain_Acc: 15.3815%\n",
            "Train Epoch: 3 [(91%)]\tTrain_Loss: 3.3234\tTrain_Acc: 15.3578%\n",
            "Train Epoch: 3 [(91%)]\tTrain_Loss: 3.3228\tTrain_Acc: 15.3666%\n",
            "Train Epoch: 3 [(92%)]\tTrain_Loss: 3.3227\tTrain_Acc: 15.3592%\n",
            "Train Epoch: 3 [(92%)]\tTrain_Loss: 3.3226\tTrain_Acc: 15.3358%\n",
            "Train Epoch: 3 [(92%)]\tTrain_Loss: 3.3213\tTrain_Acc: 15.3846%\n",
            "Train Epoch: 3 [(92%)]\tTrain_Loss: 3.3220\tTrain_Acc: 15.3772%\n",
            "Train Epoch: 3 [(93%)]\tTrain_Loss: 3.3211\tTrain_Acc: 15.3699%\n",
            "Train Epoch: 3 [(93%)]\tTrain_Loss: 3.3206\tTrain_Acc: 15.3785%\n",
            "Train Epoch: 3 [(93%)]\tTrain_Loss: 3.3203\tTrain_Acc: 15.3871%\n",
            "Train Epoch: 3 [(93%)]\tTrain_Loss: 3.3199\tTrain_Acc: 15.3877%\n",
            "Train Epoch: 3 [(94%)]\tTrain_Loss: 3.3206\tTrain_Acc: 15.3725%\n",
            "Train Epoch: 3 [(94%)]\tTrain_Loss: 3.3211\tTrain_Acc: 15.3652%\n",
            "Train Epoch: 3 [(94%)]\tTrain_Loss: 3.3212\tTrain_Acc: 15.3580%\n",
            "Train Epoch: 3 [(94%)]\tTrain_Loss: 3.3226\tTrain_Acc: 15.3274%\n",
            "Train Epoch: 3 [(95%)]\tTrain_Loss: 3.3220\tTrain_Acc: 15.3516%\n",
            "Train Epoch: 3 [(95%)]\tTrain_Loss: 3.3217\tTrain_Acc: 15.3445%\n",
            "Train Epoch: 3 [(95%)]\tTrain_Loss: 3.3212\tTrain_Acc: 15.3685%\n",
            "Train Epoch: 3 [(95%)]\tTrain_Loss: 3.3209\tTrain_Acc: 15.3614%\n",
            "Train Epoch: 3 [(95%)]\tTrain_Loss: 3.3212\tTrain_Acc: 15.3620%\n",
            "Train Epoch: 3 [(96%)]\tTrain_Loss: 3.3206\tTrain_Acc: 15.3781%\n",
            "Train Epoch: 3 [(96%)]\tTrain_Loss: 3.3209\tTrain_Acc: 15.3787%\n",
            "Train Epoch: 3 [(96%)]\tTrain_Loss: 3.3200\tTrain_Acc: 15.4100%\n",
            "Train Epoch: 3 [(96%)]\tTrain_Loss: 3.3206\tTrain_Acc: 15.4029%\n",
            "Train Epoch: 3 [(97%)]\tTrain_Loss: 3.3200\tTrain_Acc: 15.4187%\n",
            "Train Epoch: 3 [(97%)]\tTrain_Loss: 3.3196\tTrain_Acc: 15.4421%\n",
            "Train Epoch: 3 [(97%)]\tTrain_Loss: 3.3191\tTrain_Acc: 15.4653%\n",
            "Train Epoch: 3 [(97%)]\tTrain_Loss: 3.3195\tTrain_Acc: 15.4354%\n",
            "Train Epoch: 3 [(98%)]\tTrain_Loss: 3.3182\tTrain_Acc: 15.4661%\n",
            "Train Epoch: 3 [(98%)]\tTrain_Loss: 3.3178\tTrain_Acc: 15.4665%\n",
            "Train Epoch: 3 [(98%)]\tTrain_Loss: 3.3179\tTrain_Acc: 15.4744%\n",
            "Train Epoch: 3 [(98%)]\tTrain_Loss: 3.3176\tTrain_Acc: 15.4973%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
