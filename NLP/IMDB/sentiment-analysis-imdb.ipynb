{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T13:48:07.594711Z","iopub.status.busy":"2021-06-09T13:48:07.594369Z","iopub.status.idle":"2021-06-09T13:48:11.456165Z","shell.execute_reply":"2021-06-09T13:48:11.455297Z","shell.execute_reply.started":"2021-06-09T13:48:07.594636Z"},"trusted":true},"outputs":[],"source":["import numpy as np\r\n","import pandas as pd\r\n","import torch\r\n","import torchtext\r\n","import spacy\r\n","import torch\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","import torch.optim as optim\r\n","import pyprind"]},{"cell_type":"markdown","metadata":{},"source":["### The dataset can be downloaded through this link - https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format"]},{"cell_type":"markdown","metadata":{},"source":["# Loading and preprocessing the data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CreateDataset(torch.utils.data.Dataset):\r\n","\r\n","    def __init__(self, root_dir, batch_size=32):\r\n","        self.root_dir = root_dir\r\n","        self.batch_size = batch_size\r\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n","\r\n","        self.spacy = spacy.load(\"en_core_web_sm\")\r\n","\r\n","        self.TEXT = torchtext.data.Field(sequential=True, tokenize=\"spacy\")\r\n","        self.LABEL = torchtext.data.LabelField(dtype=torch.long, sequential=False)\r\n","\r\n","        self.initData()\r\n","        self.initEmbed()\r\n","\r\n","        self.makeData()\r\n","\r\n","    def initData(self):\r\n","        \r\n","        df_path = self.root_dir + 'imdb-dataset-sentiment-analysis-in-csv-format'\r\n","\r\n","        self.train_data, self.valid_data, self.test_data = torchtext.data.TabularDataset.splits(\r\n","                        path=df_path, \r\n","                        train=\"Train.csv\", validation=\"Valid.csv\", test=\"Test.csv\", \r\n","                        format=\"csv\", \r\n","                        skip_header=True, \r\n","                        fields=[('Text', self.TEXT), ('Label', self.LABEL)])\r\n","\r\n","    def initEmbed(self):\r\n","        \r\n","        embed_path = self.root_dir + 'glove6b300dtxt/glove.6B.300d.txt'\r\n","\r\n","        self.TEXT.build_vocab(self.train_data,\r\n","                         vectors=torchtext.vocab.Vectors(embed_path), \r\n","                         max_size=20000, \r\n","                         min_freq=10)\r\n","        self.LABEL.build_vocab(self.train_data)\r\n","\r\n","    def makeData(self):\r\n","        self.train_iterator, self.valid_iterator, self.test_iterator = torchtext.data.BucketIterator.splits(\r\n","                        (self.train_data, self.valid_data, self.test_data), \r\n","                        sort_key=lambda x: len(x.Text), \r\n","                        batch_size=self.batch_size,\r\n","                        device=self.device)\r\n","\r\n","    def lengthData(self):\r\n","        return len(self.train_data), len(self.valid_data), len(self.test_data)\r\n","    \r\n","    def lengthVocab(self):\r\n","        return len(self.TEXT.vocab), len(self.LABEL.vocab)\r\n","\r\n","    def freqLABEL(self):\r\n","        return self.LABEL.vocab.freqs\r\n","\r\n","    def getData(self):\r\n","        return self.train_iterator, self.valid_iterator, self.test_iterator\r\n","\r\n","    def getEmbeddings(self):\r\n","        return self.TEXT.vocab.vectors"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T13:48:11.459797Z","iopub.status.busy":"2021-06-09T13:48:11.459504Z","iopub.status.idle":"2021-06-09T13:51:02.096022Z","shell.execute_reply":"2021-06-09T13:51:02.095226Z","shell.execute_reply.started":"2021-06-09T13:48:11.459769Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","/opt/conda/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n","/opt/conda/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n","100%|█████████▉| 399999/400000 [00:54<00:00, 7362.74it/s]\n","/opt/conda/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"]},{"data":{"text/plain":"tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n        ...,\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n       device='cuda:0')"},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","dataset = CreateDataset('../input/')\n","train_iterator, valid_iterator, test_iterator = dataset.getData()\n","pretrained_embeddings = dataset.getEmbeddings()\n","pretrained_embeddings.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["# Models"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T13:51:02.098209Z","iopub.status.busy":"2021-06-09T13:51:02.097855Z","iopub.status.idle":"2021-06-09T13:51:02.110624Z","shell.execute_reply":"2021-06-09T13:51:02.109577Z","shell.execute_reply.started":"2021-06-09T13:51:02.098171Z"},"trusted":true},"outputs":[],"source":["# RNN from linear layers. Since this model only takes one input we will have to run a loop to input the words one by one.\r\n","class RNN_fc(nn.Module):\r\n","    def __init__(self, input_dim, hidden_dim, output_dim):\r\n","        super(RNN_fc, self).__init__()\r\n","        self.input_dim = input_dim\r\n","        self.hidden_dim = hidden_dim\r\n","        self.output_dim = output_dim\r\n","        self.i2h = nn.Linear(input_dim  +  hidden_dim, hidden_dim)\r\n","        self.i2o = nn.Linear(input_dim + hidden_dim, output_dim)\r\n","        self.softmax = nn.LogSoftmax(dim=1)\r\n","\r\n","    def forward(self, inputs, hidden):\r\n","        combined = torch.cat((inputs, hidden), 1)\r\n","        hidden = self.i2h(combined)\r\n","        output = self.i2o(combined)\r\n","        output = self.softmax(output)\r\n","        return output, hidden\r\n","\r\n","    def initHidden(self):\r\n","        return torch.zeros(1, self.hidden_size)    \r\n","\r\n","\r\n","# RNN using nn.Module ( got an accuracy of 86% on the test set, similar to LSTM but takes significantly lesser time to train.)\r\n","class RNN(torch.nn.Module):\r\n","    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\r\n","        super().__init__()\r\n","        \r\n","        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\r\n","        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim)\r\n","        self.linear = torch.nn.Linear(hidden_dim, output_dim)\r\n","        \r\n","    def forward(self, text):\r\n","        \r\n","        embedded = self.embedding(text)\r\n","        \r\n","        output, hidden = self.rnn(embedded)\r\n","        output = torch.mean(output, dim = 0)  # taking the average of all the ouputs after each word. This significantly improved the accuracy compared to just taking the last output \r\n","        out = self.linear(output)\r\n","        return out\r\n","\r\n","\r\n","# LSTM using nn.Module ( got an accuracy of 85% on the test set. I trained it for only 20 epohs and I think the accuracy would increase if we train it for longer)\r\n","class LSTM(torch.nn.Module): \r\n","    def __init__(self, input_dim, embedding_dim, num_layers, hidden_dim, dropout = 0.2, bidirectional = False):\r\n","        super(LSTM, self).__init__()\r\n","        self.hidden_dim = hidden_dim\r\n","        self.bidirectional = bidirectional\r\n","        self.dropout = torch.nn.Dropout(p=dropout)\r\n","\r\n","        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\r\n","        self.embedding.load_state_dict({'weight': pretrained_embeddings})\r\n","        self.embedding.weight.requires_grad = False\r\n","\r\n","        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, \r\n","                                         num_layers=num_layers,\r\n","                                         bidirectional=bidirectional,\r\n","                                         dropout=dropout)\r\n","        if bidirectional:\r\n","            self.linear = torch.nn.Linear(hidden_dim*num_layers*2, 2)\r\n","        else:\r\n","            self.linear = torch.nn.Linear(hidden_dim*num_layers, 2)\r\n","    def forward(self, text):\r\n","        embedded = self.embedding(text)\r\n","        lstm_out, (hidden, cell) = self.lstm(embedded)\r\n","        out = self.linear(self.dropout(torch.cat([cell[i,:, :] for i in range(cell.shape[0])], dim=1)))  # concatnating the cell state from all the layers and directions\r\n","        return out\r\n","\r\n"]},{"cell_type":"markdown","metadata":{},"source":["# Hyperparameters"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T13:51:02.133271Z","iopub.status.busy":"2021-06-09T13:51:02.132862Z","iopub.status.idle":"2021-06-09T13:51:03.051319Z","shell.execute_reply":"2021-06-09T13:51:03.050562Z","shell.execute_reply.started":"2021-06-09T13:51:02.133231Z"},"trusted":true},"outputs":[{"data":{"text/plain":"LSTM(\n  (dropout): Dropout(p=0.2, inplace=False)\n  (embedding): Embedding(20002, 300)\n  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n  (linear): Linear(in_features=1024, out_features=2, bias=True)\n)"},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["input_dim = dataset.lengthVocab()[0]\r\n","embedding_dim = 300\r\n","hidden_dim = 256\r\n","output_dim = 2\r\n","num_layers = 2\r\n","batch_size = 32\r\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n","model = LSTM(input_dim, embedding_dim, num_layers, hidden_dim, bidirectional = True)  # using a bidirectional 2-layer lstm\r\n","model.to(device)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T13:51:03.054417Z","iopub.status.busy":"2021-06-09T13:51:03.054174Z","iopub.status.idle":"2021-06-09T13:51:03.062394Z","shell.execute_reply":"2021-06-09T13:51:03.061591Z","shell.execute_reply.started":"2021-06-09T13:51:03.054392Z"},"trusted":true},"outputs":[],"source":["optimizer = optim.SGD(model.parameters(), lr=1e-3)\r\n","criterion = nn.CrossEntropyLoss()\r\n","model = model.to(device)\r\n","criterion = criterion.to(device)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T13:51:03.073764Z","iopub.status.busy":"2021-06-09T13:51:03.073169Z","iopub.status.idle":"2021-06-09T13:51:03.089931Z","shell.execute_reply":"2021-06-09T13:51:03.089028Z","shell.execute_reply.started":"2021-06-09T13:51:03.073725Z"},"trusted":true},"outputs":[],"source":["def accuracy(preds, y):\r\n","\r\n","    preds, ind= torch.max(F.softmax(preds, dim=-1), 1)\r\n","    correct = (ind == y).float()\r\n","    acc = correct.sum()/float(len(correct))\r\n","    return acc\r\n","\r\n","def train(model, iterator, optimizer, criterion):\r\n","    \r\n","    epoch_loss = 0\r\n","    epoch_acc = 0\r\n","    \r\n","    model.train()\r\n","    bar = pyprind.ProgBar(len(iterator), bar_char='█')\r\n","    for batch in iterator:\r\n","        \r\n","        optimizer.zero_grad()\r\n","                \r\n","        predictions = model(batch.Text).squeeze(0)\r\n","\r\n","        loss = criterion(predictions, batch.Label)\r\n","\r\n","        acc = accuracy(predictions, batch.Label)\r\n","        \r\n","        loss.backward()\r\n","        \r\n","        optimizer.step()\r\n","        \r\n","        epoch_loss += loss.item()\r\n","        epoch_acc += acc.item()\r\n","        bar.update()\r\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\r\n","\r\n","def evaluate(model, iterator, criterion):\r\n","    \r\n","    epoch_loss = 0\r\n","    epoch_acc = 0\r\n","    \r\n","    model.eval()\r\n","    \r\n","    with torch.no_grad():\r\n","        for batch in iterator:\r\n","\r\n","            predictions = model(batch.Text).squeeze(0)\r\n","            \r\n","            loss = criterion(predictions, batch.Label)\r\n","            \r\n","            acc = accuracy(predictions, batch.Label)\r\n","\r\n","            epoch_loss += loss.item()\r\n","            epoch_acc += acc.item()\r\n","            \r\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"]},{"cell_type":"markdown","metadata":{},"source":["# Training the LSTM model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T13:51:03.091791Z","iopub.status.busy":"2021-06-09T13:51:03.091311Z","iopub.status.idle":"2021-06-09T16:03:12.429207Z","shell.execute_reply":"2021-06-09T16:03:12.428309Z","shell.execute_reply.started":"2021-06-09T13:51:03.091675Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n","  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:29\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1 \t Train Loss: 0.689  \t Train Acc: 54.02% \n","Val. Loss: 0.686 \t Val. Acc: 53.74% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:30\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 2 \t Train Loss: 0.681  \t Train Acc: 57.61% \n","Val. Loss: 0.678 \t Val. Acc: 56.55% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:29\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 3 \t Train Loss: 0.669  \t Train Acc: 60.03% \n","Val. Loss: 0.664 \t Val. Acc: 59.65% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:30\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 4 \t Train Loss: 0.647  \t Train Acc: 62.47% \n","Val. Loss: 0.634 \t Val. Acc: 64.23% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:29\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 5 \t Train Loss: 0.619  \t Train Acc: 65.48% \n","Val. Loss: 0.578 \t Val. Acc: 69.53% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:28\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 6 \t Train Loss: 0.601  \t Train Acc: 68.09% \n","Val. Loss: 0.544 \t Val. Acc: 75.10% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:30\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 7 \t Train Loss: 0.657  \t Train Acc: 60.43% \n","Val. Loss: 0.686 \t Val. Acc: 53.03% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:31\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 8 \t Train Loss: 0.692  \t Train Acc: 53.72% \n","Val. Loss: 0.680 \t Val. Acc: 55.06% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:34\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 9 \t Train Loss: 0.679  \t Train Acc: 57.06% \n","Val. Loss: 0.663 \t Val. Acc: 60.43% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:32\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 10 \t Train Loss: 0.660  \t Train Acc: 60.43% \n","Val. Loss: 0.669 \t Val. Acc: 59.36% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:31\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 11 \t Train Loss: 0.642  \t Train Acc: 63.84% \n","Val. Loss: 0.651 \t Val. Acc: 63.95% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:35\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 12 \t Train Loss: 0.632  \t Train Acc: 64.80% \n","Val. Loss: 0.572 \t Val. Acc: 71.95% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:32\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 13 \t Train Loss: 0.612  \t Train Acc: 67.00% \n","Val. Loss: 0.536 \t Val. Acc: 73.71% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:33\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 14 \t Train Loss: 0.592  \t Train Acc: 69.10% \n","Val. Loss: 0.660 \t Val. Acc: 63.38% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:30\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 15 \t Train Loss: 0.576  \t Train Acc: 69.80% \n","Val. Loss: 0.453 \t Val. Acc: 80.25% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:33\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 16 \t Train Loss: 0.461  \t Train Acc: 79.66% \n","Val. Loss: 0.420 \t Val. Acc: 81.53% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:33\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 17 \t Train Loss: 0.405  \t Train Acc: 82.54% \n","Val. Loss: 0.388 \t Val. Acc: 82.76% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:30\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 18 \t Train Loss: 0.373  \t Train Acc: 84.25% \n","Val. Loss: 0.345 \t Val. Acc: 85.49% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:35\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 19 \t Train Loss: 0.358  \t Train Acc: 84.95% \n","Val. Loss: 0.342 \t Val. Acc: 85.67% \n"]},{"name":"stderr","output_type":"stream","text":["0% [██████████████████████████████] 100% | ETA: 00:00:00\n","Total time elapsed: 00:06:31\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 20 \t Train Loss: 0.349  \t Train Acc: 85.47% \n","Val. Loss: 0.340 \t Val. Acc: 85.51% \n"]}],"source":["epochs = 20\n","best_acc = 0\n","for epoch in range(epochs):\n","\n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    if valid_acc > best_acc:\n","        torch.save(model.state_dict(), 'weights_lstm_sentiment.pth')\n","    print(f'Epoch: {epoch+1} \\t Train Loss: {train_loss:.3f}  \\t Train Acc: {train_acc*100:.2f}% \\nVal. Loss: {valid_loss:.3f} \\t Val. Acc: {valid_acc*100:.2f}% ')\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T16:03:12.431136Z","iopub.status.busy":"2021-06-09T16:03:12.430765Z","iopub.status.idle":"2021-06-09T16:03:17.013896Z","shell.execute_reply":"2021-06-09T16:03:17.013090Z","shell.execute_reply.started":"2021-06-09T16:03:12.431098Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Val. Loss: 0.358 \t Val. Acc: 84.75% \n"]}],"source":["test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","print(f'Val. Loss: {test_loss:.3f} \\t Val. Acc: {test_acc*100:.2f}% ')"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2021-06-09T16:03:17.015585Z","iopub.status.busy":"2021-06-09T16:03:17.015246Z","iopub.status.idle":"2021-06-09T16:05:26.498331Z","shell.execute_reply":"2021-06-09T16:05:26.497443Z","shell.execute_reply.started":"2021-06-09T16:03:17.015536Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["train. Loss: 0.353 \t train. Acc: 85.10% \n","Val. Loss: 0.340 \t Val. Acc: 85.51% \n","test. Loss: 0.358 \t test. Acc: 84.75% \n"]}],"source":["model.load_state_dict(torch.load('./weights_lstm_sentiment.pth'))\n","train_loss, train_acc = evaluate(model, train_iterator, criterion)\n","print(f'train. Loss: {train_loss:.3f} \\t train. Acc: {train_acc*100:.2f}% ')\n","val_loss, val_acc = evaluate(model, valid_iterator, criterion)\n","print(f'Val. Loss: {val_loss:.3f} \\t Val. Acc: {val_acc*100:.2f}% ')\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","print(f'test. Loss: {test_loss:.3f} \\t test. Acc: {test_acc*100:.2f}% ')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.5 64-bit ('base': conda)","name":"python385jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"},"language_info":{"name":"python","version":""}},"nbformat":4,"nbformat_minor":4}